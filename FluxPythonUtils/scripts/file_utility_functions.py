# Standard imports
import os
import logging
import pickle
import threading
from typing import List, Dict, Type, TypeVar
import yaml
from pathlib import PurePath, Path
import csv
from datetime import datetime
import subprocess
import json
import getpass

# 3rd party packages
import pandas as pd
import polars as pl
from orjson import orjson

# FluxPythonUtils Modules
from FluxPythonUtils.scripts.yaml_importer import YAMLImporter
from FluxPythonUtils.scripts.model_base_utils import MsgspecBaseModel
from FluxPythonUtils.scripts.file_n_general_utility_functions import (
    pandas_df_to_model_obj_list, polars_df_to_model_obj_list, LOG_FORMAT, file_exist)

MsgspecModel = TypeVar('MsgspecModel', bound=MsgspecBaseModel)


class EmptyFileError(Exception):
    """Exception raised for unexpected empty file.
    Attributes:
        file_or_dir_path -- file path that was found with empty file
        message -- brief explanation of the error (file path passed is auto-appended f-str below)
    """

    def __init__(self, file_path: str, message: str = "Empty file found!"):
        self.file_path = file_path
        self.message = message
        super().__init__(f"{self.message}, file_path: {self.file_path}")


def csv_to_xlsx(file_name: str, csv_data_dir: PurePath | None = None, xlsx_data_dir: PurePath | None = None,
                col_format_dict: Dict[int, str] | None = None):
    if csv_data_dir is None:
        csv_data_dir = PurePath(__file__).parent / "data"
    if xlsx_data_dir is None:
        xlsx_data_dir = csv_data_dir
    csv_path = PurePath(csv_data_dir / f"{file_name}.csv")
    xlsx_path = PurePath(xlsx_data_dir / f"{file_name}.xlsx")
    csv_as_df = pd.read_csv(str(csv_path))
    xlsx_writer = pd.ExcelWriter(str(xlsx_path), engine="xlsxwriter")
    sheet_name = "target"
    csv_as_df.to_excel(xlsx_writer, index=False, header=True, sheet_name=sheet_name)
    if col_format_dict:
        workbook = xlsx_writer.book
        ws = xlsx_writer.sheets[sheet_name]
        ws.autofit()
        notional_format = workbook.add_format({'num_format': '$  ###,##0'})
        dollar_format = workbook.add_format({'num_format': '$  #,##0.00'})
        percent_format = workbook.add_format({'num_format': '0.00 %'})
        bold_italic_format = workbook.add_format({'bold': True, 'italic': True})
        for col_pos, col_format in col_format_dict.items():
            if col_format == "dollar":
                ws.set_column(col_pos, col_pos, 10, dollar_format)
            elif col_format == "notional":
                ws.set_column(col_pos, col_pos, 12, notional_format)
            elif col_format == "percent":
                ws.set_column(col_pos, col_pos, 6, percent_format)
            elif col_format == "bold_italic_format":
                ws.set_column(col_pos, col_pos, None, bold_italic_format)
            else:
                logging.warning(f"ignoring unsupported col_format: {col_format} passed for col: {col_pos}")
    xlsx_writer.close()


def get_fieldnames_from_record_type(record_type) -> List[str]:
    keys_to_exclude = ["is_time_series", "enable_large_db_object"]
    fieldnames = [key for key in record_type.__annotations__.keys() if key not in keys_to_exclude]
    return fieldnames


def dict_or_list_records_csv_writer_ext(file_name: str, records: Dict | List, record_type,
                                        data_dir: PurePath | None = None):
    fieldnames = get_fieldnames_from_record_type(record_type)
    dict_or_list_records_csv_writer(file_name, records, fieldnames, record_type, data_dir)


def dict_or_list_records_csv_writer(file_name: str, records: Dict | List, fieldnames, record_type,
                                    data_dir: PurePath | None = None, append_mode: bool = False,
                                    include_header: bool = True, by_alias: bool = False):
    """
    :param append_mode: (bool) append records in csv file if exists, else create a new csv file
    :param include_header: (bool) write the header if set to True, else ignore
    :param by_alias: (bool) if True, records are stored by alias name
    - fieldnames generated by model_json_schema has by_alias=True by default
    - records generated by model_dump and model_dump_json has by_alias=False by default
    fieldnames can be subset of fields you wish to write in CSV
    constraints:
    1. records can be collection of Dict or List
    2. specific record MUST support dict() method
    """
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    csv_path = PurePath(data_dir / f"{file_name}.csv")
    mode: str = "w"  # write mode
    if append_mode:
        mode = "a"  # append mode
    with open(csv_path, mode, encoding="utf-8", newline='') as fp:
        writer = csv.DictWriter(fp, fieldnames=fieldnames, extrasaction='ignore')
        if include_header:
            writer.writeheader()
        if isinstance(records, Dict):
            for record in records.values():
                json_dict = record.to_dict()
                writer.writerow(json_dict)
        elif isinstance(records, List):
            for record in records:
                json_dict = record.to_dict()
                writer.writerow(json_dict)
        else:
            raise Exception(
                f"Unexpected: Un-supported type passed, expected Dict or List found type: {type(records)} of "
                f"object: {str(records)}")


def get_csv_path_from_name_n_dir(file_name: str, data_dir: PurePath | None = None):
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    return PurePath(data_dir / f"{file_name}.csv")


def pandas_to_polars_dtypes(pandas_dtypes: dict) -> dict:
    """
    Convert Pandas dtype dictionary to Polars compatible dtypes/schema_overrides.
    Args:
        pandas_dtypes (dict): Dictionary of column names and their Pandas dtypes
    Returns:
        dict: Dictionary of column names and their corresponding Polars dtypes
    Example:
        pandas_dtypes = {
            'col1': 'int64',
            'col2': 'float64',
            'col3': 'object',
            'col4': 'datetime64[ns]',
            'col5': 'bool'
        }
        polars_dtypes = pandas_to_polars_dtypes(pandas_dtypes)
    """
    # Mapping of pandas dtypes to polars dtypes
    dtype_mapping = {
        # Numeric types
        'int32': pl.Int32,
        'int64': pl.Int64,
        'float32': pl.Float32,
        'float64': pl.Float64,
        # String types
        'object': pl.Utf8,
        'string': pl.Utf8,
        'str': pl.Utf8,
        # Boolean type
        'bool': pl.Boolean,
        # DateTime types
        'datetime64[ns]': pl.Datetime,
        'datetime64': pl.Datetime,
        'timedelta64[ns]': pl.Duration,
        'timedelta64': pl.Duration,
    }
    polars_dtypes = {}
    for column, dtype in pandas_dtypes.items():
        # Convert dtype to string format if it's not already
        dtype_str = str(dtype)
        # Handle numpy dtype strings
        if dtype_str.startswith('datetime64'):
            polars_dtypes[column] = pl.Datetime
        elif dtype_str.startswith('timedelta64'):
            polars_dtypes[column] = pl.Duration
        else:
            # Remove any additional information (like [ns])
            base_dtype = dtype_str.split('[')[0]
            # Get corresponding polars dtype or default to Utf8
            polars_dtypes[column] = dtype_mapping.get(base_dtype, pl.Utf8)
    return polars_dtypes


def dict_or_list_records_csv_reader(file_name: str, MsgspecType: Type[MsgspecBaseModel],
                                    data_dir: PurePath | None = None, rename_col_names_to_snake_case: bool = False,
                                    rename_col_names_to_lower_case: bool = True,
                                    no_throw: bool = False,
                                    dtype: Dict | None = None) -> List[MsgspecModel]:
    """
    At this time the method only supports list of msgspec_type extraction from csv
    """
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    if not file_name.endswith(".csv"):
        file_name = f"{file_name}.csv"
    csv_path = data_dir / file_name
    str_csv_path = str(csv_path)
    if os.path.exists(str_csv_path) and os.path.getsize(str_csv_path) > 0:
        return dict_or_list_records_polars_csv_reader(MsgspecType, csv_path, rename_col_names_to_snake_case,
                                                      rename_col_names_to_lower_case, dtype)
    elif not no_throw:
        raise Exception(f"dict_or_list_records_csv_reader invoked on empty or no csv file: {str_csv_path}")
    return []


def dict_or_list_records_pandas_csv_reader(MsgspecType: Type[MsgspecBaseModel], csv_path: PurePath | None = None,
                                           rename_col_names_to_snake_case: bool = False,
                                           rename_col_names_to_lower_case: bool = True,
                                           dtype: Dict | None = None) -> List[MsgspecModel]:
    """
    At this time the method only supports list of msgspec_type extraction form csv using pandas
    """
    # by setting keep_default_na=False, pandas will ignore its built-in list of default strings that are
    # normally recognized as missing values.
    if dtype is not None:
        read_df = pd.read_csv(csv_path, keep_default_na=False, dtype=dtype)
    else:
        read_df = pd.read_csv(csv_path, keep_default_na=False)
    return pandas_df_to_model_obj_list(read_df, MsgspecType, rename_col_names_to_snake_case,
                                       rename_col_names_to_lower_case)


def dict_or_list_records_polars_csv_reader(MsgspecType: Type[MsgspecBaseModel], csv_path: PurePath | None = None,
                                           rename_col_names_to_snake_case: bool = False,
                                           rename_col_names_to_lower_case: bool = True,
                                           dtype: Dict | None = None) -> List[MsgspecModel]:
    """
    At this time the method only supports list of msgspec_type extraction form csv using polars
    """
    # null_values tells Polars not to treat any values as null unless you explicitly specify them. In
    # dict_or_list_records_pandas_csv_reader we do similar with keep_default_na=False
    # default infer_schema_length is 100
    if dtype is not None:
        polars_dtype = pandas_to_polars_dtypes(dtype)
        read_df = pl.read_csv(str(csv_path), null_values=[], infer_schema_length=10000, schema_overrides=polars_dtype)
    else:
        read_df = pl.read_csv(str(csv_path), null_values=[], infer_schema_length=10000)
    return polars_df_to_model_obj_list(read_df, MsgspecType, rename_col_names_to_snake_case,
                                       rename_col_names_to_lower_case)


def str_from_file(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8", newline='') as fp:
        return fp.read()


def store_json_or_dict_to_file(file_name: str, json_dict, data_dir: PurePath | None = None):
    # Serialize json dict
    json_str = json.dumps(json_dict, indent=4)
    # write to json file
    store_json_str_to_file(file_name, json_str, data_dir)


def store_json_str_to_file(file_name: str, json_str, data_dir: PurePath | None = None, mode="w"):
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    json_file_path = PurePath(data_dir / f"{file_name}.json")
    with open(json_file_path, mode) as outfile:
        outfile.write(json_str)


def store_str_list_to_file(file_name: str, str_list: List[str], data_dir: PurePath | None = None, no_ext: bool = True,
                           mode="w", separator="\n"):
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    file_path: PurePath
    if no_ext:
        file_path = PurePath(data_dir / f"{file_name}")
    else:
        file_path = PurePath(data_dir / f"{file_name}.txt")

    with open(file_path, mode) as outfile:
        str_: str
        for str_ in str_list:
            outfile.write(str_ + separator)


def load_json_dict_from_file(file_name: str, data_dir: PurePath | None = None, must_exist: bool = True):
    if not file_name.endswith(".json"):
        if data_dir is None:
            data_dir = PurePath(__file__).parent / "data"
        # else use passed data dir
        json_file_path = PurePath(data_dir / f"{file_name}.json")
    else:  # file name passed is complete file path
        json_file_path = file_name  # filename has path and suffix
    if (not must_exist) and (not file_exist(str(json_file_path))):
        return None
    if os.path.getsize(json_file_path) > 0:
        # Open JSON file
        json_file_obj = open(json_file_path)
        # return JSON object as a dictionary
        json_dict = json.load(json_file_obj)
        return json_dict
    else:
        raise EmptyFileError(json_file_path, f"load_json_dict_from_file: json file found, but its empty!")


def get_match_file_from_path(file_name_prefix: str, file_name_suffix: str, file_store_root_dir_path: PurePath):
    matched_files: List[any] = list()
    files_matched = (Path(file_store_root_dir_path)).glob(f"{file_name_prefix}*{file_name_suffix}")
    if files_matched is not None:
        for filename in files_matched:
            matched_files.append(filename)
    return matched_files


def archive_match_files(file_name_prefix: str, file_store_root_dir_path: PurePath,
                        ignore_files: List | None = None, clear_archive: bool = False):
    # check match files exist
    files_to_archive = (Path(file_store_root_dir_path)).glob(f"{file_name_prefix}*")
    if files_to_archive is not None:
        # delete old archive
        archive_pure_path = file_store_root_dir_path / "archive"
        archive_path = Path(archive_pure_path)
        if not Path.exists(archive_path):
            # create archive dir if it does not exist
            os.mkdir(str(archive_path))
        else:
            # cleanup since archive exists
            # remove all file with matching prefixes
            if clear_archive:
                files_to_delete = archive_path.glob(f"{file_name_prefix}*")
                for filename in files_to_delete:
                    os.remove(str(filename))
        # move files_to_archive to archive
        for filename in files_to_archive:
            if ignore_files is None or (os.path.basename(filename) not in ignore_files):
                archive_filename = archive_pure_path / os.path.basename(filename)
                os.rename(filename, archive_filename)


def store_to_pickle_file(file_name: str, python_object, data_dir: PurePath | None = None, mode='wb'):
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    pickle_file_path = PurePath(data_dir / f"{file_name}.pickle")
    with open(pickle_file_path, mode) as fp:
        pickle.dump(python_object, fp)


def load_from_pickle_file(file_name: str, data_dir: PurePath | None = None, mode='rb'):
    if data_dir is None:
        data_dir = PurePath(__file__).parent / "data"
    pickle_file_path = PurePath(data_dir / f"{file_name}.pickle")
    if file_exist(str(pickle_file_path)):
        with open(pickle_file_path, mode) as fp:
            file_content = fp.read()
            python_object = pickle.loads(bytes(file_content))
            return python_object
    else:
        return None  # file not found


def makedir(path: str | PurePath) -> None:
    """
    Function to make directory. Takes complete path as input argument.
    """
    os.mkdir(path if isinstance(path, str) else str(path))


def delete_file(path: str | PurePath) -> None:
    """
    Function to delete file. Takes complete path as input argument.
    """
    os.remove(path if isinstance(path, str) else str(path))


def encrypt_file(file_path: str, encrypted_file_path: str | None = None):
    if not file_exist(file_path):
        raise Exception(f"encrypt_file failed, file to be encrypted does not exist, {file_path=}")
    if not encrypted_file_path:
        encrypted_file_path = f"{file_path}.enc"
    # else not required - encrypted file path is set
    password = getpass.getpass(prompt='Enter passphrase for encryption: ')
    try:
        with subprocess.Popen(
                ['openssl', 'enc', '-aes-256-cbc', '-salt', '-in', file_path, '-out', encrypted_file_path,
                 '-pass', 'stdin'],
                stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:
            process.communicate(input=password.encode())
        logging.debug(f"File '{file_path}' encrypted to '{encrypted_file_path}'")
    except subprocess.CalledProcessError as e:
        logging.error(f"encrypt_file failed: {e}")


def decrypt_file(file_path: str, decrypted_file_path: str | None = None):
    if not file_exist(file_path):
        raise Exception(f"encrypt_file failed, file to be encrypted does not exist, {file_path=}")
    if not decrypted_file_path:
        if file_path.endswith(".enc"):
            decrypted_file_path = file_path.replace(".enc", "")
        else:
            raise Exception("decrypt_file failed, decrypted_file_path not provided and file_path does not end with "
                            "'.enc'")
    # else not required - decrypted file path is set
    password = getpass.getpass(prompt='Enter passphrase for decryption: ')
    try:
        with subprocess.Popen(
                ['openssl', 'enc', '-d', '-aes-256-cbc', '-salt', '-in', file_path, '-out', decrypted_file_path,
                 '-pass', 'stdin'],
                stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:
            process.communicate(input=password.encode())
        logging.debug(f"File '{file_path}' decrypted to '{decrypted_file_path}'")
    except subprocess.CalledProcessError as e:
        logging.error(f"decrypt_file failed: {e}")


def logical_split_file(destination: str, content: str, max_size: int):
    """
    Splits content into multiple files if it exceeds max_size while ensuring logical splits (e.g., newline boundaries).

    :param destination: The base path of the file to be written.
    :param content: The content to be written.
    :param max_size: The maximum size for each file in bytes (default: 50MB).
    :return: List of generated file paths.
    """
    base_dir = os.path.dirname(destination)
    base_name, ext = os.path.splitext(os.path.basename(destination))
    content_bytes = content.encode('utf-8')

    start = 0
    file_count = 0
    generated_files = []

    while start < len(content_bytes):
        # Find a safe split point before max_size
        end = min(start + max_size, len(content_bytes))
        if end < len(content_bytes):
            # Ensure logical break by looking for the last newline before the max_size limit
            last_newline = content_bytes.rfind(b'\n', start, end)
            if last_newline != -1:
                end = last_newline + 1  # Include the newline

        # Extract the chunk
        chunk = content_bytes[start:end]

        # Determine filename
        if file_count == 0 and end >= len(content_bytes):
            filename = destination  # Single file case
        else:
            filename = os.path.join(base_dir, f"{base_name}_part{file_count + 1}{ext}")

        # Write to file
        with open(filename, 'wb') as f:
            f.write(chunk)

        # Store the generated file path
        generated_files.append(filename)

        # Move to the next part
        start = end
        file_count += 1

    return generated_files
